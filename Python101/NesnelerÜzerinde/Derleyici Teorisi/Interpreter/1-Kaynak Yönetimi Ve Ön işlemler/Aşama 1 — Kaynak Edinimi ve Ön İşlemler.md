# ğŸ§© 1. AÅAMA â€” KAYNAK EDÄ°NÄ°MÄ° & Ã–N Ä°ÅLEMLER


---
#### ğŸ§‘â€ğŸ“ Yeni baÅŸlayan (CS Ã¶ÄŸrencisi)
#### ğŸ•’ **Tahmini SÃ¼re*: 90dk*

---

Python yorumlayÄ±cÄ±sÄ± **(Interpreter-CPython)** bir `.py` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmaya baÅŸladÄ±ÄŸÄ±nda,  
ilk yaptÄ±ÄŸÄ± ÅŸey aslÄ±nda **dosyayÄ± aÃ§mak** deÄŸil, **onu anlamlandÄ±rmaya hazÄ±rlamak**<sup>[1]</sup> olur.

Bu aÅŸamanÄ±n amacÄ±; diskteki **ham baytlarÄ±<sup>[2]</sup> (bytes)** alÄ±p,  
Pythonâ€™un **lexer<sup>[3]</sup>**â€™Ä±nÄ±n (ya da tokenizerâ€™Ä±nÄ±n) iÅŸleyebileceÄŸi **standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ Unicode<sup>[4]</sup> metin** haline getirmektir.

HenÃ¼z â€œtokenâ€, â€œASTâ€, â€œbytecodeâ€ yoktur.  
Sadece **ham veriler (baytlar)** ve onlarÄ± Pythonâ€™un anlayabileceÄŸi biÃ§ime Ã§evirme sÃ¼reci vardÄ±r.

---

**[1]** Ham baytlarÄ± doÄŸru decodingâ€™le Unicodeâ€™a Ã§evirip BOMâ€™u temizleyerek, satÄ±r sonlarÄ±nÄ± `\n`â€™e normalize edip lexerâ€™Ä±n kullanacaÄŸÄ± satÄ±r-bazlÄ± akÄ±ÅŸÄ± oluÅŸturma.  

**[2]** **Ham bayt**, bir dosyada karakterlerin encodingâ€™e gÃ¶re temsil edildiÄŸi 8-bitlik veri birimleridir. Python yorumlayÄ±cÄ±sÄ± bu baytlarÄ± doÄŸrudan anlamaz; Ã¶nce encoding bilgisiyle decode edip Unicode karakterlere dÃ¶nÃ¼ÅŸtÃ¼rmesi gerekir.

**[3]** **Lexer**, kaynak kodu bayt dÃ¼zeyinden Unicode karakter dizisine (string) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ metin olarak alÄ±r ve yalnÄ±zca bu karakterlerle Ã§alÄ±ÅŸarak dilin sÃ¶zdizimsel yapÄ±larÄ±nÄ± tanÄ±mlar.

**[4]** **Unicode**, dÃ¼nya Ã¼zerindeki tÃ¼m yazÄ± sistemlerindeki karakterleri temsil etmek iÃ§in geliÅŸtirilmiÅŸ evrensel bir standarttÄ±r.
Her karaktere benzersiz bir kod noktasÄ± (Ã¶rneÄŸin U+0061 â†’ 'a') atar

>âœ… **Encoding**, Unicode karakterleri bayt dizilerine dÃ¶nÃ¼ÅŸtÃ¼ren protokoldÃ¼r; dosya yazÄ±mÄ± ve aÄŸ iletiÅŸimi iÃ§in gereklidir.bu karakterlerin bayt dizisi olarak nasÄ±l saklandÄ±ÄŸÄ±nÄ± belirtir. 

>âœ… **Decode**, bayt dizilerini belirli bir encoding'e gÃ¶re Unicode karakterlere Ã§evirerek metin haline getirir.**Encoding bilinmeden, baytlar Unicodeâ€™a Ã§evrilemez â†’ lexer Ã§alÄ±ÅŸamaz.**

---

## ğŸ¯ AmaÃ§

Pythonâ€™un diskteki dosyayÄ± alÄ±p, **platformdan baÄŸÄ±msÄ±z, temiz, normalize edilmiÅŸ bir kaynak metin** oluÅŸturmasÄ±dÄ±r.

Lexer **karakterlerle** Ã§alÄ±ÅŸÄ±r, **baytlarla** deÄŸil.
O yÃ¼zden Pythonâ€™un bu aÅŸamadaki temel hedefi, **ham bayt akÄ±ÅŸÄ±nÄ± karakter tabanlÄ± bir akÄ±ÅŸa Ã§evirmek**tir.

Yani:

- Kodlama (encoding) bilgisini bulmak
- Gizli baytlarÄ± (Ã¶rneÄŸin BOM) temizlemek
- SatÄ±r sonlarÄ±nÄ± normalize etmek
- SatÄ±r bazlÄ± bir okuma mekanizmasÄ± (`readline` akÄ±ÅŸÄ±) hazÄ±rlamak

Bu aÅŸama tamamlanmadan lexer tek bir token bile Ã¼retemez.

---

## ğŸ§  Python Bu Ä°ÅŸi Nerede Yapar?

Bu sÃ¼reÃ§ CPythonâ€™Ä±n iÃ§inde, Ã§oÄŸunlukla ÅŸu C kaynak dosyalarÄ±nda gerÃ§ekleÅŸir:

- ğŸ§© **`Python/tokenizer.c`** â†’ AsÄ±l â€œtokenizerâ€ motoru burada baÅŸlar, ama Ã¶nce kaynak metin hazÄ±rlanÄ±r.
- ğŸ§© **`Parser/tokenizer_pgen.c`** â†’ Eski parser iÃ§in, ancak aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±r.
- ğŸ§© **`Python/pythonrun.c`** â†’ Pythonâ€™un Ã§alÄ±ÅŸtÄ±rma dÃ¶ngÃ¼sÃ¼nÃ¼ (`run loop`) yÃ¶netir, dosyayÄ± aÃ§ar, akÄ±ÅŸÄ± oluÅŸturur.

Bu iÅŸlemler doÄŸrudan ÅŸu fonksiyonlar Ã¼zerinden Ã§aÄŸrÄ±lÄ±r:

- `PyParser_ASTFromFileObject()`
- `PyRun_InteractiveOneObject()`

---

## ğŸª„ 1.1 â€” DosyanÄ±n OkunmasÄ± (Bayt DÃ¼zeyinde)

Ä°lk aÅŸama tamamen **dosya sistemi** dÃ¼zeyindedir.
Python kaynak dosyayÄ± **binary modda (`rb`)** aÃ§ar â€” Ã§Ã¼nkÃ¼ dosya sisteminde her ÅŸey **bayt** olarak saklanÄ±r.

```python
Ã¶rnek_dosya = open("main.py", "rb")
veri = Ã¶rnek_dosya.read()
print(veri[:20])  # b'# -*- coding: utf-8 -*-'
```

### C dÃ¼zeyinde bu iÅŸlemi baÅŸlatan fonksiyonlar:

- `PyParser_ASTFromFileObject()` â†’ DosyayÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in
- `PyRun_InteractiveOneObject()` â†’ REPL ortamÄ±nda satÄ±r satÄ±r okumak iÃ§in

---

## ğŸ” 1.2 â€” Kodlama (Encoding) Tespiti â€” PEP 263

Python, kaynaÄŸÄ±n hangi karakter setiyle yazÄ±ldÄ±ÄŸÄ±nÄ± anlamalÄ±dÄ±r.
Her `.py` dosyasÄ± aslÄ±nda Unicode karakterlerden oluÅŸan bir metin olmalÄ±dÄ±r.

### PEP 263â€™e gÃ¶re:

- DosyanÄ±n ilk iki satÄ±rÄ± geÃ§ici olarak **ASCII<sup>[1]</sup>** kabul edilir
- Bu satÄ±rlarda encoding bildirimi aranÄ±r:

- **Konum**: Encoding bildirimi **yalnÄ±zca ilk iki satÄ±rda** geÃ§erlidir.
- **BiÃ§im**: `coding[:=]\s*([-\w.]+)`
- **HatalÄ± cookie Ã¶rneÄŸi**:
```python
# coding utf8   # iki nokta/eÅŸittir yok â†’ GeÃ§ersiz

# -*- coding: utf-8 -*-
# coding: latin-1
```
**[1]** **ASCII**, bilgisayarlarÄ±n metin karakterlerini sayÄ±sal baytlarla temsil etmesini saÄŸlayan 7 bitlik evrensel bir karakter kodlama sistemidir.encoding bildirimi ASCII karakterlerle yazÄ±ldÄ±ÄŸÄ±nda, yorumlayÄ±cÄ± onu encoding bilgisi olmadan bile okuyabilir

> ğŸ’¡ Not: **Kabul edilmesi** demek, eÄŸer bu iki satÄ±rda **ASCII** olmayan bir metin varsa hata ver deÄŸil sadece bu satÄ±rlarÄ± **ASCII** formatÄ±nda okur.


**Uyumsuzluk Ã–rneÄŸi:**  
Dosya baÅŸlÄ±ÄŸÄ±: `# coding: latin-1`  
Ä°Ã§erik: UTF-8 TÃ¼rkÃ§e karakterler (`ÅŸ`, `ÄŸ`)  
â†’ Decode sÄ±rasÄ±nda `UnicodeDecodeError` veya sonrasÄ±nda `SyntaxError` gÃ¶rÃ¼lebilir.  
**Ã‡Ã¶zÃ¼m:** DoÄŸru encodingâ€™i bildir veya kaynak dosyayÄ± uygun encoding ile kaydet.

## âš¡ 1.3 â€” BOM (Byte Order Mark) KontrolÃ¼

BazÄ± Unicode dosyalarÄ±nÄ±n baÅŸÄ±nda Ã¶zel bir iÅŸaret bulunur:

Bu iÅŸaret **BOM (Byte Order Mark)** olarak adlandÄ±rÄ±lÄ±r. Kodun bir parÃ§asÄ± deÄŸildir, ama dosyanÄ±n Unicode olduÄŸunu belirtir.

### EÄŸer BOM temizlenmezse:

```python
'\ufeffprint("selam")'
```
Lexer bunu geÃ§ersiz karakter olarak algÄ±lar:
```python
SyntaxError: invalid character in identifier
```

## âœ… BOM Tablosu (Byte Order Mark)

Unicode dosyalarÄ±nÄ±n baÅŸÄ±nda yer alabilen BOM (Byte Order Mark) imzalarÄ±, dosyanÄ±n encoding tÃ¼rÃ¼nÃ¼ ve byte sÄ±rasÄ±nÄ± belirtir. Python, bu imzalarÄ± tanÄ±r ve lexer baÅŸlamadan Ã¶nce temizler.

| ğŸ”¤ Encoding     | ğŸ”¢ BOM (Hex)        | ğŸ“Œ AÃ§Ä±klama                          |
|----------------|---------------------|--------------------------------------|
| **UTF-8**      | `EF BB BF`          | Opsiyonel; varsa atÄ±lÄ±r              |
| **UTF-16 LE**  | `FF FE`             | Little-endian sÄ±ralama belirtir     |
| **UTF-16 BE**  | `FE FF`             | Big-endian sÄ±ralama belirtir        |
| **UTF-32 LE**  | `FF FE 00 00`       | Little-endian sÄ±ralama belirtir     |
| **UTF-32 BE**  | `00 00 FE FF`       | Big-endian sÄ±ralama belirtir        |

> ğŸ’¡ Not: Python, BOM'u gÃ¶rdÃ¼ÄŸÃ¼nde encoding'i otomatik olarak belirler ve bu baytlarÄ± lexer'a geÃ§meden Ã¶nce temizler. Ã–zellikle UTF-8 BOM (`EF BB BF`) yaygÄ±n olarak metin editÃ¶rleri tarafÄ±ndan eklenir, ancak Python tarafÄ±ndan kodun parÃ§asÄ± olarak kabul edilmez.


## ğŸ” 1.4 â€” SatÄ±r SonlarÄ±nÄ±n Normalize Edilmesi

Pythonâ€™un lexerâ€™Ä± (tokenizer) kaynak kodu iÅŸlerken satÄ±r bazÄ±nda Ã§alÄ±ÅŸÄ±r â€” Ã§Ã¼nkÃ¼ dilin yapÄ±sÄ±nda **girinti (indentation)<sup>[1]</sup>**, **blok sÄ±nÄ±rlarÄ±** ve **ifade sonlarÄ±** hep â€œsatÄ±r sonuâ€na baÄŸlÄ±dÄ±r. Ancak farklÄ± iÅŸletim sistemleri satÄ±r sonlarÄ±nÄ± farklÄ± karakterlerle belirtir:

- **Windows:** `\r\n` (Carriage Return + Line Feed)  
- **Unix / Linux / macOS:** `\n` (Line Feed)  
- **Klasik Mac:** `\r` (Carriage Return)

Bu fark, eÄŸer normalize edilmezse lexerâ€™Ä±n â€œsatÄ±r bitti mi?â€ sorusuna tutarsÄ±z cevaplar vermesine yol aÃ§abilir.  
Ã–rneÄŸin `\r\n` iÃ§eren bir kaynakta Python `\r` karakterini gÃ¶rÃ¼nmez, ama token sÄ±nÄ±rÄ± zannedip hatalÄ± bir `NEWLINE` Ã¼retebilir.

Bu yÃ¼zden CPython, **kaynak metin lexerâ€™a ulaÅŸmadan Ã¶nce** tÃ¼m satÄ±r sonlarÄ±nÄ± **tek biÃ§ime (`\n`)** dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.  
Bu iÅŸlem â€œsatÄ±r sonu normalizasyonuâ€ olarak adlandÄ±rÄ±lÄ±r.

Bu sayede:
- `INDENT`, `DEDENT` ve `NEWLINE` tokenâ€™larÄ± her platformda aynÄ± ÅŸekilde tanÄ±mlanÄ±r,  
- Kod taÅŸÄ±nabilir hale gelir (Windowsâ€™ta yazÄ±lÄ±p Linuxâ€™ta da Ã§alÄ±ÅŸÄ±r),  
- Parser ve AST Ã¼retimi platformdan baÄŸÄ±msÄ±z olur.

Bu adÄ±m, **dilin girinti tabanlÄ± doÄŸasÄ±** nedeniyle kritik Ã¶nemdedir;  
Pythonâ€™un sÃ¶zdizimsel bÃ¼tÃ¼nlÃ¼ÄŸÃ¼, bu normalize edilmiÅŸ satÄ±r sÄ±nÄ±rlarÄ±na dayanÄ±r.


deâ€™a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi, Pythonâ€™un kaynak kodu anlamlandÄ±rma sÃ¼recinin ilk ve en kritik adÄ±mÄ±dÄ±r. Bu dÃ¶nÃ¼ÅŸÃ¼m sayesinde lexer, dilin gramerine uygun ÅŸekilde token Ã¼retmeye baÅŸlayabilir. Encoding tespiti, BOM temizliÄŸi ve satÄ±r normalizasyonu gibi alt adÄ±mlar bu sÃ¼recin ayrÄ±lmaz parÃ§alarÄ±dÄ±r. Pythonâ€™un Unicode temelli mimarisi bu adÄ±mÄ± zorunlu kÄ±lar.
>âœ… **Girinti:** Pythonâ€™da sÃ¼slÃ¼ parantezler yerine, kod bloklarÄ±nÄ± tanÄ±mlamak iÃ§in kullanÄ±lan boÅŸluk veya tab karakterlerinden oluÅŸan yapÄ±sal bir iÅŸarettir
```python
# Girinti;
def greek():
    pass # burda 4 boÅŸluk var
```

---
>âœ… **Blok SÄ±nÄ±rÄ±:** Girinti seviyesinin deÄŸiÅŸmesiyle belirlenen yapÄ±sal geÃ§iÅŸ noktasÄ±dÄ±r
```python
# Blok SÄ±nÄ±rlarÄ±
for _ in range(10): # iki nokta for bloÄŸunun baÅŸladÄ±ÄŸÄ±nÄ± belirtir
    pass
```
---
>âœ… **Ä°fade Sonu:** Her Python ifadesi satÄ±r sonuyla (\n) veya noktalÄ± virgÃ¼lle (;) sonlanÄ±r
```python
# Ä°fade SonlarÄ±
name = "demir" # ifade sonu
age = 20 # ifade sonu
id = 12 ; city = istanbul # birden fazla ifadeyi : ile ayÄ±rdÄ±k
```
---
>ğŸ’¡ Python'da sÃ¼slÃ¼ parantezler yerine girintiler blok yapÄ±sÄ±nÄ± tanÄ±mlar; bu yÃ¼zden **lexer**, satÄ±r sonlarÄ±nÄ± ve boÅŸluk karakterlerini yapÄ±sal ayrÄ±m iÃ§in semantik olarak iÅŸler. Kaynak edinimi sÄ±rasÄ±nda satÄ±r sonlarÄ±nÄ±n normalize edilmemesi, **lexerâ€™Ä±n** girinti seviyelerini yanlÄ±ÅŸ yorumlamasÄ±na ve `IndentationError` gibi yapÄ±sal hatalara yol aÃ§abilir.


### âš™ï¸ CPython DÃ¼zeyinde SatÄ±r SonlarÄ±nÄ±n Normalize Edilmesi

SatÄ±r sonlarÄ±nÄ±n normalize edilmesi iÅŸlemi, Python yorumlayÄ±cÄ±sÄ±nÄ±n (CPython) **tokenizer** modÃ¼lÃ¼nde, yani `Python/tokenizer.c` dosyasÄ±nda gerÃ§ekleÅŸir.  
Bu iÅŸlem **lexer baÅŸlamadan hemen Ã¶nce**, kaynak metin â€œokuma akÄ±ÅŸÄ±â€ (readline stream) haline getirilirken yapÄ±lÄ±r.

### ğŸ“œ Ne Zaman Oluyor?

ğŸ” Bunu anlamak iÃ§in veri akÄ±ÅŸÄ±nÄ± ÅŸÃ¶yle dÃ¼ÅŸÃ¼nebilirsin:
```scss
Diskteki dosya  â”€â”€>  open("file.py", "rb")  â”€â”€>  FILE*  â”€â”€>  
_PyTokenizer_FromFile()  â”€â”€>  tok_nextc()  â”€â”€>  Lexer (tok_get)
```
---

## ğŸ“œ 1.5 â€” SatÄ±r BazlÄ± Okuma AkÄ±ÅŸÄ± (Readline Stream) HazÄ±rlanmasÄ±

Pythonâ€™un **lexerâ€™Ä± (tokenizer)**, kaynak kodu bir bÃ¼tÃ¼n olarak deÄŸil, **satÄ±r satÄ±r iÅŸler**. Bunun temel nedeni, Pythonâ€™un sÃ¶zdiziminin (syntax) doÄŸrudan satÄ±r yapÄ±sÄ±na dayanmasÄ±dÄ±r:

- Her satÄ±r bir `NEWLINE` tokenâ€™Ä± ile sonlanÄ±r.
- Girintiler `INDENT` / `DEDENT` tokenâ€™larÄ± ile temsil edilir ve bu girintiler satÄ±r baÅŸlarÄ±ndaki boÅŸluklarla belirlenir.
- REPL gibi etkileÅŸimli ortamlarda kullanÄ±cÄ± her satÄ±r girdiÄŸinde ayrÄ± bir analiz yapÄ±lÄ±r.

DolayÄ±sÄ±yla, Pythonâ€™un lexerâ€™Ä± **satÄ±r temelli bir akÄ±ÅŸ (stream)** olmadan Ã§alÄ±ÅŸamaz. Bu akÄ±ÅŸ, kaynak kodun satÄ±r satÄ±r okunmasÄ±nÄ± ve her satÄ±rÄ±n baÄŸÄ±msÄ±z olarak tokenâ€™lara ayrÄ±lmasÄ±nÄ± saÄŸlar. SatÄ±r bazlÄ± okuma, hem girinti mantÄ±ÄŸÄ±nÄ± hem de etkileÅŸimli analizleri mÃ¼mkÃ¼n kÄ±lar.

---

## ğŸ¯ AmaÃ§

Bu aÅŸamanÄ±n hedefi:

> â€œNormalize edilmiÅŸ Unicode metni, satÄ±r satÄ±r okunabilir bir veri kaynaÄŸÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmek.â€

Python, kaynak kodu bir kerede hafÄ±zaya yÃ¼klemek yerine, **her seferinde bir satÄ±r okuyan bir arabirim (readline)** oluÅŸturur. Bu yaklaÅŸÄ±m, Pythonâ€™un satÄ±r temelli sÃ¶zdizimiyle doÄŸrudan Ã¶rtÃ¼ÅŸÃ¼r.

---

### ğŸ§­ Lexer Bu AkÄ±ÅŸla Ne Yapar?

- Her seferinde `readline()` Ã§aÄŸÄ±rarak yeni bir satÄ±r alÄ±r.
- SatÄ±rdaki karakterleri tek tek sÄ±nÄ±flandÄ±rÄ±r.
- SatÄ±r sonuna ulaÅŸtÄ±ÄŸÄ±nda, o satÄ±ra ait token dizisini tamamlar.

Bu yapÄ± sayesinde lexer, hem girinti mantÄ±ÄŸÄ±nÄ± hem de etkileÅŸimli ortamlarÄ± (REPL gibi) doÄŸru ÅŸekilde iÅŸleyebilir.

---

### âš™ï¸ CPython DÃ¼zeyinde Ne Olur?

Bu mekanizma, C dÃ¼zeyinde `tokenizer.c` dosyasÄ±nda tanÄ±mlanmÄ±ÅŸ `_PyTokenizer_FromFile()` fonksiyonuyla baÅŸlatÄ±lÄ±r.

- Bu fonksiyon, lexerâ€™a **â€œnasÄ±l satÄ±r okunacaÄŸÄ±nÄ±â€** sÃ¶yleyen arabirimi saÄŸlar.
- Kaynak dosya, satÄ±r satÄ±r okunabilir hale getirilir.
- Tokenizer, bu arabirim Ã¼zerinden satÄ±rlarÄ± alarak analiz eder.

Bu yapÄ±, Pythonâ€™un REPL davranÄ±ÅŸÄ±nÄ±, girinti mantÄ±ÄŸÄ±nÄ± ve satÄ±r sonu tokenizasyonunu mÃ¼mkÃ¼n kÄ±lan temel mimari bileÅŸendir.

---

## ğŸ§± Mimari YapÄ±

Python lexerâ€™Ä±nÄ±n satÄ±r bazlÄ± Ã§alÄ±ÅŸabilmesi iÃ§in, her kaynak akÄ±ÅŸÄ±nÄ±n iÃ§inde bir **â€œokuma fonksiyonuâ€** bulunur. Bu mimari, lexerâ€™a satÄ±r satÄ±r veri saÄŸlayan soyut bir arabirim oluÅŸturur.

### ğŸ”Œ C DÃ¼zeyinde AkÄ±ÅŸ FonksiyonlarÄ±

- `_PyTokenizer_FromFile()` â†’ `fp_readline()`  
- `_PyTokenizer_FromString()` â†’ `string_readline()`

Bu fonksiyonlar, **pointer olarak `tok_nextc()` fonksiyonuna verilir**.  
`tok_nextc()` her Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda:

- EÄŸer mevcut satÄ±r bitmiÅŸse,
- `readline()` fonksiyonu ile yeni bir satÄ±r alÄ±nÄ±r.


---
### ğŸ“¦ Bellek Dostu ve EtkileÅŸimli TasarÄ±m

Bu yapÄ± sayesinde lexer:

- DosyayÄ± **asla tamamÄ±nÄ± okumaz**.
- YalnÄ±zca ihtiyaÃ§ oldukÃ§a yeni satÄ±r getirir.
- REPL gibi etkileÅŸimli ortamlarda **her satÄ±r ayrÄ± ayrÄ± iÅŸlenebilir**.

Bu mimari, hem bellek verimliliÄŸi saÄŸlar hem de REPL gibi satÄ±r bazlÄ± analiz gerektiren ortamlarÄ± destekler.

---

### ğŸ” Python DÃ¼zeyinde KarÅŸÄ±lÄ±ÄŸÄ±

AynÄ± mantÄ±k Python tarafÄ±nda da gÃ¶zlemlenebilir:

```python
import io

kaynak = "x = 10\nprint(x)\n"
akÄ±ÅŸ = io.StringIO(kaynak)

print(akÄ±ÅŸ.readline())  # 'x = 10\n'
print(akÄ±ÅŸ.readline())  # 'print(x)\n'
```

>âœ…  Bu basit Ã¶rnek, lexerâ€™Ä±n kullandÄ±ÄŸÄ± â€œstream abstractionâ€Ä±n Pythonâ€™daki karÅŸÄ±lÄ±ÄŸÄ±dÄ±r.
 Her readline() Ã§aÄŸrÄ±sÄ± bir satÄ±r dÃ¶ndÃ¼rÃ¼r,
 ve lexer bu satÄ±rdaki karakterleri tek tek analiz eder.

---

## ğŸ§© Neden SatÄ±r BazlÄ±?

Python, **girinti tabanlÄ± (indentation-sensitive)** bir dildir.  
Yani:

- Kod bloklarÄ± `{}` yerine **girintilerle** belirlenir.
- Bu da lexerâ€™Ä±n **satÄ±r sÄ±nÄ±rlarÄ±nÄ± mutlak biÃ§imde bilmesini** zorunlu kÄ±lar.
- `readline()` akÄ±ÅŸÄ±, Python dilinin sÃ¶zdizimsel doÄŸasÄ±yla **birebir uyumludur**.
- Her satÄ±r, dilin **yapÄ±sal birimi** haline gelir.

Bu mimari, hem dilin semantiÄŸi hem de yorumlayÄ±cÄ±nÄ±n Ã§alÄ±ÅŸma ÅŸekli aÃ§Ä±sÄ±ndan kritik Ã¶nemdedir.

### ğŸ’¡ Avantajlar

|ğŸ§© Ã–zellik                     | ğŸ“Œ AÃ§Ä±klama                                                                     |
|-----------------------------|---------------------------------------------------------------------------------|
| ğŸ§  Bellek dostu             | KaynaÄŸÄ±n tamamÄ±nÄ± deÄŸil, **satÄ±r satÄ±r iÅŸler**.                                 |
| ğŸ“ Hata konumlama kolaylÄ±ÄŸÄ± | SatÄ±r numaralarÄ± (`lineno`) **akÄ±ÅŸtan otomatik hesaplanÄ±r**.                    |
| ğŸ’¬ REPL desteÄŸi             | KullanÄ±cÄ±dan gelen **her satÄ±r anÄ±nda lexerâ€™a verilebilir**.                    |
| ğŸŒ Platform baÄŸÄ±msÄ±zlÄ±ÄŸÄ±    | SatÄ±r sonlarÄ± normalize edildiÄŸi iÃ§in (`\n`), **tÃ¼m sistemlerde aynÄ±** Ã§alÄ±ÅŸÄ±r. |

---
### âš™ï¸ NasÄ±l Ã‡alÄ±ÅŸÄ±r?

Python l**exerâ€™Ä±**, kaynak kodu Unicode karakterler hÃ¢line getirdikten sonra **satÄ±r bazlÄ± bir okuma akÄ±ÅŸÄ±** Ã¼zerinden iÅŸlemeye baÅŸlar. Bu akÄ±ÅŸ, hem C dÃ¼zeyinde hem de Python dÃ¼zeyinde benzer mantÄ±kla iÅŸler.

### ğŸ”¹ 1. AkÄ±ÅŸÄ±n OluÅŸturulmasÄ±

- Kaynak kod artÄ±k **normalize edilmiÅŸ Unicode metin** hÃ¢lindedir.
- Bu metin, `readline()` benzeri bir **akÄ±ÅŸ arabirimi** ile lexerâ€™a sunulur.

### ğŸ› ï¸ C DÃ¼zeyinde AkÄ±ÅŸ MantÄ±ÄŸÄ±

| Fonksiyon               | GÃ¶rev                                                                 |
|------------------------|-----------------------------------------------------------------------|
| `fp_readline()`        | Her Ã§aÄŸrÄ±da bir satÄ±r okur.                                           |
| `tok_state.buf`        | Okunan satÄ±r bu bufferâ€™a yerleÅŸtirilir.                               |
| `tok_nextc()`          | Bufferâ€™dan karakter karakter okuma iÅŸlemini gerÃ§ekleÅŸtirir.           |

Bu yapÄ± sayesinde lexer, **satÄ±r sonlarÄ±nÄ±**, **girinti seviyelerini** ve **token sÄ±nÄ±rlarÄ±nÄ±** doÄŸru ÅŸekilde analiz edebilir.

### ğŸ Python DÃ¼zeyinde KarÅŸÄ±lÄ±ÄŸÄ±

```python
import io

kaynak = "x = 10\nprint(x)\n"
akÄ±ÅŸ = io.StringIO(kaynak)

print(akÄ±ÅŸ.readline())  # 'x = 10\n'
print(akÄ±ÅŸ.readline())  # 'print(x)\n'
```

---

### ğŸ”¹ 2. SatÄ±r SÄ±nÄ±rlarÄ±nÄ±n Belirlenmesi

Python lexerâ€™Ä±, her satÄ±r sonunda Ã¶zel tokenâ€™lar Ã¼retir. Bu tokenâ€™lar, dilin yapÄ±sal mantÄ±ÄŸÄ±nÄ± tanÄ±mlamak iÃ§in kritik Ã¶nemdedir:

| Token     | AÃ§Ä±klama                                                                 |
|-----------|--------------------------------------------------------------------------|
| `NEWLINE` | MantÄ±ksal bir satÄ±rÄ±n sonunu belirtir.                                   |
| `NL`      | Fiziksel satÄ±r sonu (Ã¶rneÄŸin parantez iÃ§inde kesilen satÄ±rlar).          |
| `INDENT`  | Yeni girinti seviyesi (Ã¶rneÄŸin `def`, `if`, `for` bloklarÄ±).             |
| `DEDENT`  | Girinti azalmasÄ± (blok sonu).                                            |

Bu tokenâ€™lar **Pythonâ€™a Ã¶zgÃ¼dÃ¼r** â€” C, Java veya JavaScript gibi dillerde bu tÃ¼r yapÄ±sal girinti tokenâ€™larÄ± bulunmaz.

---

### ğŸ”¹ 3. Girintilerin (Indentation) Ã–lÃ§Ã¼lmesi

Lexer, her satÄ±rÄ±n baÅŸÄ±ndaki boÅŸluk veya tab karakterlerini sayarak girinti mantÄ±ÄŸÄ±nÄ± uygular:

- Yeni satÄ±rÄ±n girinti seviyesi **Ã¶ncekinden bÃ¼yÃ¼kse** â†’ `INDENT`
- **KÃ¼Ã§Ã¼kse** â†’ bir veya daha fazla `DEDENT`
- **AynÄ±ysa** â†’ hiÃ§bir ÅŸey Ã¼retmez

#### âš™ï¸ C DÃ¼zeyinde Girinti Analizi

Bu iÅŸlem `tok_get()` fonksiyonu iÃ§inde gerÃ§ekleÅŸir. Her satÄ±rÄ±n baÅŸÄ±nda lexer:

- `tok->line_start` konumundan itibaren boÅŸluklarÄ± okur.
- Girinti yÄ±ÄŸÄ±nÄ±nÄ± (`indent_stack`) gÃ¼nceller.
- Uygun sayÄ±da `INDENT` veya `DEDENT` tokenâ€™Ä± Ã¼retir.

Bu yapÄ±, Pythonâ€™un girinti tabanlÄ± sÃ¶zdizimini doÄŸru ÅŸekilde analiz edebilmesi iÃ§in zorunludur.

---

### ğŸ”¹ 4. Bu YapÄ± Neden `readline()`â€™a BaÄŸlÄ±?

Ã‡Ã¼nkÃ¼ lexer, **satÄ±rlarÄ± tek tek deÄŸerlendirmelidir**.  
EÄŸer tÃ¼m dosya bir defada okunsaydÄ±:

- SatÄ±r sonlarÄ±nÄ±, girintileri ve blok sÄ±nÄ±rlarÄ±nÄ± **yÃ¶netmek imkÃ¢nsÄ±z olurdu**.
- Ã–zellikle REPL (etkileÅŸimli mod) **doÄŸru Ã§alÄ±ÅŸmazdÄ±**.

#### âœ… `readline()` AkÄ±ÅŸÄ±nÄ±n SaÄŸladÄ±klarÄ±

- SatÄ±r bazlÄ± iÅŸleme
- Realtime (anlÄ±k) Ã§alÄ±ÅŸma
- Dosya ve REPL iÃ§in **ortak mantÄ±k**

Bu mimari, Pythonâ€™un hem dosya tabanlÄ± hem de etkileÅŸimli yorumlama yeteneklerini mÃ¼mkÃ¼n kÄ±lar.

---

### ğŸ§© CPython Ä°Ã§inde Neler Olur?

C dÃ¼zeyinde bu iÅŸlemler sÄ±rasÄ±yla ÅŸu ÅŸekilde gerÃ§ekleÅŸir:

| ğŸ› ï¸ Fonksiyon                | ğŸ¯ GÃ¶rev                                                               |
|---------------------------|----------------------------------------------------------------------|
| `_PyTokenizer_FromFile()` | `tok_state` yapÄ±sÄ±nÄ± oluÅŸturur, `readline` fonksiyonunu referanslar. |
| `fp_readline()`           | Câ€™nin `fgets()` fonksiyonunu kullanarak satÄ±rÄ± okur.                 |
| `tok_nextc()`             | Bufferâ€™dan karakter karakter okur.                                   |
| `tok_get()`               | SatÄ±r baÅŸÄ±ndaki girinti miktarÄ±nÄ± Ã¶lÃ§er, `INDENT` / `DEDENT` Ã¼retir. |

Bu yapÄ±, lexerâ€™Ä±n satÄ±r bazlÄ± ve girinti duyarlÄ± Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlayan temel mimari bileÅŸenlerdir.

---

### ğŸ’¡ Neden Bu Kadar Kritik?

Ã‡Ã¼nkÃ¼ Pythonâ€™da **girinti yalnÄ±zca gÃ¶rsel bir biÃ§im deÄŸildir** â€”  
dil gramerinin **semantik parÃ§asÄ±dÄ±r**.

Ã–rneÄŸin:

```python
def Ã¶rnek():
    if True:
        print("merhaba")
    print("bitti")
```
#### Lexer bunu ÅŸu sÄ±rayla tokenize eder:
```scss
NAME('def'), NAME('Ã¶rnek'), LPAR, RPAR, COLON, NEWLINE,
INDENT,
NAME('if'), NAME('True'), COLON, NEWLINE,
INDENT,
NAME('print'), ... , NEWLINE,
DEDENT,
NAME('print'), ... , NEWLINE,
DEDENT,
ENDMARKER
```
> âœ… **`INDENT/DEDENT`**â†’ kod bloklarÄ±nÄ± tanÄ±mlar

> âœ… **`NEWLINE`** â†’ ifadeleri sÄ±nÄ±rlar

> ğŸ’¡ Bu sistem olmadan Pythonâ€™un sÃ¶zdizimi `def`, `if`, `while` bloklarÄ± mÃ¼mkÃ¼n olmazdÄ±.
**KÄ±sacasÄ±: readline akÄ±ÅŸÄ±, Python dilinin kalp atÄ±ÅŸÄ±dÄ±r.**