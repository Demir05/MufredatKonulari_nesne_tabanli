# ğŸ§© 1. AÅAMA â€” KAYNAK EDÄ°NÄ°MÄ° & Ã–N Ä°ÅLEMLER

Python yorumlayÄ±cÄ±sÄ± **(Interpreter-CPython)** bir `.py` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmaya baÅŸladÄ±ÄŸÄ±nda,  
ilk yaptÄ±ÄŸÄ± ÅŸey aslÄ±nda **dosyayÄ± aÃ§mak** deÄŸil, **onu anlamlandÄ±rmaya hazÄ±rlamak**<sup>[1]</sup> olur.

Bu aÅŸamanÄ±n amacÄ±; diskteki **ham baytlarÄ±<sup>[2]</sup> (bytes)** alÄ±p,  
Pythonâ€™un **lexer<sup>[3]</sup>**â€™Ä±nÄ±n (ya da tokenizerâ€™Ä±nÄ±n) iÅŸleyebileceÄŸi **standartlaÅŸtÄ±rÄ±lmÄ±ÅŸ Unicode<sup>[4]</sup> metin** haline getirmektir.

HenÃ¼z â€œtokenâ€, â€œASTâ€, â€œbytecodeâ€ yoktur.  
Sadece **ham veriler (baytlar)** ve onlarÄ± Pythonâ€™un anlayabileceÄŸi biÃ§ime Ã§evirme sÃ¼reci vardÄ±r.

---

**[1]** Ham baytlarÄ± doÄŸru decodingâ€™le Unicodeâ€™a Ã§evirip BOMâ€™u temizleyerek, satÄ±r sonlarÄ±nÄ± `\n`â€™e normalize edip lexerâ€™Ä±n kullanacaÄŸÄ± satÄ±r-bazlÄ± akÄ±ÅŸÄ± oluÅŸturma.  

**[2]** **Ham bayt**, bir dosyada karakterlerin encodingâ€™e gÃ¶re temsil edildiÄŸi 8-bitlik veri birimleridir. Python yorumlayÄ±cÄ±sÄ± bu baytlarÄ± doÄŸrudan anlamaz; Ã¶nce encoding bilgisiyle decode edip Unicode karakterlere dÃ¶nÃ¼ÅŸtÃ¼rmesi gerekir.

**[3]** **Lexer**, kaynak kodu bayt dÃ¼zeyinden Unicode karakter dizisine (string) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ metin olarak alÄ±r ve yalnÄ±zca bu karakterlerle Ã§alÄ±ÅŸarak dilin sÃ¶zdizimsel yapÄ±larÄ±nÄ± tanÄ±mlar.

**[4]** **Unicode**, dÃ¼nya Ã¼zerindeki tÃ¼m yazÄ± sistemlerindeki karakterleri temsil etmek iÃ§in geliÅŸtirilmiÅŸ evrensel bir standarttÄ±r.
Her karaktere benzersiz bir kod noktasÄ± (Ã¶rneÄŸin U+0061 â†’ 'a') atar

>âœ… **Encoding**, Unicode karakterleri bayt dizilerine dÃ¶nÃ¼ÅŸtÃ¼ren protokoldÃ¼r; dosya yazÄ±mÄ± ve aÄŸ iletiÅŸimi iÃ§in gereklidir.bu karakterlerin bayt dizisi olarak nasÄ±l saklandÄ±ÄŸÄ±nÄ± belirtir. 

>âœ… **Decode**, bayt dizilerini belirli bir encoding'e gÃ¶re Unicode karakterlere Ã§evirerek metin haline getirir.**Encoding bilinmeden, baytlar Unicodeâ€™a Ã§evrilemez â†’ lexer Ã§alÄ±ÅŸamaz.**

---

## ğŸ¯ AmaÃ§

Pythonâ€™un diskteki dosyayÄ± alÄ±p, **platformdan baÄŸÄ±msÄ±z, temiz, normalize edilmiÅŸ bir kaynak metin** oluÅŸturmasÄ±dÄ±r.

Lexer **karakterlerle** Ã§alÄ±ÅŸÄ±r, **baytlarla** deÄŸil.
O yÃ¼zden Pythonâ€™un bu aÅŸamadaki temel hedefi, **ham bayt akÄ±ÅŸÄ±nÄ± karakter tabanlÄ± bir akÄ±ÅŸa Ã§evirmek**tir.

Yani:

- Kodlama (encoding) bilgisini bulmak
- Gizli baytlarÄ± (Ã¶rneÄŸin BOM) temizlemek
- SatÄ±r sonlarÄ±nÄ± normalize etmek
- SatÄ±r bazlÄ± bir okuma mekanizmasÄ± (`readline` akÄ±ÅŸÄ±) hazÄ±rlamak

Bu aÅŸama tamamlanmadan lexer tek bir token bile Ã¼retemez.

---

## ğŸ§  Python Bu Ä°ÅŸi Nerede Yapar?

Bu sÃ¼reÃ§ CPythonâ€™Ä±n iÃ§inde, Ã§oÄŸunlukla ÅŸu C kaynak dosyalarÄ±nda gerÃ§ekleÅŸir:

- ğŸ§© **`Python/tokenizer.c`** â†’ AsÄ±l â€œtokenizerâ€ motoru burada baÅŸlar, ama Ã¶nce kaynak metin hazÄ±rlanÄ±r.
- ğŸ§© **`Parser/tokenizer_pgen.c`** â†’ Eski parser iÃ§in, ancak aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±r.
- ğŸ§© **`Python/pythonrun.c`** â†’ Pythonâ€™un Ã§alÄ±ÅŸtÄ±rma dÃ¶ngÃ¼sÃ¼nÃ¼ (`run loop`) yÃ¶netir, dosyayÄ± aÃ§ar, akÄ±ÅŸÄ± oluÅŸturur.

Bu iÅŸlemler doÄŸrudan ÅŸu fonksiyonlar Ã¼zerinden Ã§aÄŸrÄ±lÄ±r:

- `PyParser_ASTFromFileObject()`
- `PyRun_InteractiveOneObject()`

---

## ğŸª„ 1.1 â€” DosyanÄ±n OkunmasÄ± (Bayt DÃ¼zeyinde)

Ä°lk aÅŸama tamamen **dosya sistemi** dÃ¼zeyindedir.
Python kaynak dosyayÄ± **binary modda (`rb`)** aÃ§ar â€” Ã§Ã¼nkÃ¼ dosya sisteminde her ÅŸey **bayt** olarak saklanÄ±r.

```python
Ã¶rnek_dosya = open("main.py", "rb")
veri = Ã¶rnek_dosya.read()
print(veri[:20])  # b'# -*- coding: utf-8 -*-'
```

### C dÃ¼zeyinde bu iÅŸlemi baÅŸlatan fonksiyonlar:

- `PyParser_ASTFromFileObject()` â†’ DosyayÄ± Ã§alÄ±ÅŸtÄ±rmak iÃ§in
- `PyRun_InteractiveOneObject()` â†’ REPL ortamÄ±nda satÄ±r satÄ±r okumak iÃ§in

---

## ğŸ” 1.2 â€” Kodlama (Encoding) Tespiti â€” PEP 263

Python, kaynaÄŸÄ±n hangi karakter setiyle yazÄ±ldÄ±ÄŸÄ±nÄ± anlamalÄ±dÄ±r.
Her `.py` dosyasÄ± aslÄ±nda Unicode karakterlerden oluÅŸan bir metin olmalÄ±dÄ±r.

### PEP 263â€™e gÃ¶re:

- DosyanÄ±n ilk iki satÄ±rÄ± geÃ§ici olarak **ASCII<sup>[1]</sup>** kabul edilir
- Bu satÄ±rlarda encoding bildirimi aranÄ±r:

- **Konum**: Encoding bildirimi **yalnÄ±zca ilk iki satÄ±rda** geÃ§erlidir.
- **BiÃ§im**: `coding[:=]\s*([-\w.]+)`
- **HatalÄ± cookie Ã¶rneÄŸi**:
```python
# coding utf8   # iki nokta/eÅŸittir yok â†’ GeÃ§ersiz

# -*- coding: utf-8 -*-
# coding: latin-1
```
**[1]** **ASCII**, bilgisayarlarÄ±n metin karakterlerini sayÄ±sal baytlarla temsil etmesini saÄŸlayan 7 bitlik evrensel bir karakter kodlama sistemidir.encoding bildirimi ASCII karakterlerle yazÄ±ldÄ±ÄŸÄ±nda, yorumlayÄ±cÄ± onu encoding bilgisi olmadan bile okuyabilir

> ğŸ’¡ Not: **Kabul edilmesi** demek, eÄŸer bu iki satÄ±rda **ASCII** olmayan bir metin varsa hata ver deÄŸil sadece bu satÄ±rlarÄ± **ASCII** formatÄ±nda okur.


**Uyumsuzluk Ã–rneÄŸi:**  
Dosya baÅŸlÄ±ÄŸÄ±: `# coding: latin-1`  
Ä°Ã§erik: UTF-8 TÃ¼rkÃ§e karakterler (`ÅŸ`, `ÄŸ`)  
â†’ Decode sÄ±rasÄ±nda `UnicodeDecodeError` veya sonrasÄ±nda `SyntaxError` gÃ¶rÃ¼lebilir.  
**Ã‡Ã¶zÃ¼m:** DoÄŸru encodingâ€™i bildir veya kaynak dosyayÄ± uygun encoding ile kaydet.

## âš¡ 1.3 â€” BOM (Byte Order Mark) KontrolÃ¼

BazÄ± Unicode dosyalarÄ±nÄ±n baÅŸÄ±nda Ã¶zel bir iÅŸaret bulunur:

Bu iÅŸaret **BOM (Byte Order Mark)** olarak adlandÄ±rÄ±lÄ±r. Kodun bir parÃ§asÄ± deÄŸildir, ama dosyanÄ±n Unicode olduÄŸunu belirtir.

### EÄŸer BOM temizlenmezse:

```python
'\ufeffprint("selam")'
```
Lexer bunu geÃ§ersiz karakter olarak algÄ±lar:
```python
SyntaxError: invalid character in identifier
```

## âœ… BOM Tablosu (Byte Order Mark)

Unicode dosyalarÄ±nÄ±n baÅŸÄ±nda yer alabilen BOM (Byte Order Mark) imzalarÄ±, dosyanÄ±n encoding tÃ¼rÃ¼nÃ¼ ve byte sÄ±rasÄ±nÄ± belirtir. Python, bu imzalarÄ± tanÄ±r ve lexer baÅŸlamadan Ã¶nce temizler.

| ğŸ”¤ Encoding     | ğŸ”¢ BOM (Hex)        | ğŸ“Œ AÃ§Ä±klama                          |
|----------------|---------------------|--------------------------------------|
| **UTF-8**      | `EF BB BF`          | Opsiyonel; varsa atÄ±lÄ±r              |
| **UTF-16 LE**  | `FF FE`             | Little-endian sÄ±ralama belirtir     |
| **UTF-16 BE**  | `FE FF`             | Big-endian sÄ±ralama belirtir        |
| **UTF-32 LE**  | `FF FE 00 00`       | Little-endian sÄ±ralama belirtir     |
| **UTF-32 BE**  | `00 00 FE FF`       | Big-endian sÄ±ralama belirtir        |

> ğŸ’¡ Not: Python, BOM'u gÃ¶rdÃ¼ÄŸÃ¼nde encoding'i otomatik olarak belirler ve bu baytlarÄ± lexer'a geÃ§meden Ã¶nce temizler. Ã–zellikle UTF-8 BOM (`EF BB BF`) yaygÄ±n olarak metin editÃ¶rleri tarafÄ±ndan eklenir, ancak Python tarafÄ±ndan kodun parÃ§asÄ± olarak kabul edilmez.


## ğŸ” 1.4 â€” SatÄ±r SonlarÄ±nÄ±n Normalize Edilmesi

Pythonâ€™un lexerâ€™Ä± (tokenizer) kaynak kodu iÅŸlerken satÄ±r bazÄ±nda Ã§alÄ±ÅŸÄ±r â€” Ã§Ã¼nkÃ¼ dilin yapÄ±sÄ±nda **girinti (indentation)<sup>[1]</sup>**, **blok sÄ±nÄ±rlarÄ±** ve **ifade sonlarÄ±** hep â€œsatÄ±r sonuâ€na baÄŸlÄ±dÄ±r. Ancak farklÄ± iÅŸletim sistemleri satÄ±r sonlarÄ±nÄ± farklÄ± karakterlerle belirtir:

- **Windows:** `\r\n` (Carriage Return + Line Feed)  
- **Unix / Linux / macOS:** `\n` (Line Feed)  
- **Klasik Mac:** `\r` (Carriage Return)

Bu fark, eÄŸer normalize edilmezse lexerâ€™Ä±n â€œsatÄ±r bitti mi?â€ sorusuna tutarsÄ±z cevaplar vermesine yol aÃ§abilir.  
Ã–rneÄŸin `\r\n` iÃ§eren bir kaynakta Python `\r` karakterini gÃ¶rÃ¼nmez, ama token sÄ±nÄ±rÄ± zannedip hatalÄ± bir `NEWLINE` Ã¼retebilir.

Bu yÃ¼zden CPython, **kaynak metin lexerâ€™a ulaÅŸmadan Ã¶nce** tÃ¼m satÄ±r sonlarÄ±nÄ± **tek biÃ§ime (`\n`)** dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.  
Bu iÅŸlem â€œsatÄ±r sonu normalizasyonuâ€ olarak adlandÄ±rÄ±lÄ±r.

Bu sayede:
- `INDENT`, `DEDENT` ve `NEWLINE` tokenâ€™larÄ± her platformda aynÄ± ÅŸekilde tanÄ±mlanÄ±r,  
- Kod taÅŸÄ±nabilir hale gelir (Windowsâ€™ta yazÄ±lÄ±p Linuxâ€™ta da Ã§alÄ±ÅŸÄ±r),  
- Parser ve AST Ã¼retimi platformdan baÄŸÄ±msÄ±z olur.

Bu adÄ±m, **dilin girinti tabanlÄ± doÄŸasÄ±** nedeniyle kritik Ã¶nemdedir;  
Pythonâ€™un sÃ¶zdizimsel bÃ¼tÃ¼nlÃ¼ÄŸÃ¼, bu normalize edilmiÅŸ satÄ±r sÄ±nÄ±rlarÄ±na dayanÄ±r.


deâ€™a dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi, Pythonâ€™un kaynak kodu anlamlandÄ±rma sÃ¼recinin ilk ve en kritik adÄ±mÄ±dÄ±r. Bu dÃ¶nÃ¼ÅŸÃ¼m sayesinde lexer, dilin gramerine uygun ÅŸekilde token Ã¼retmeye baÅŸlayabilir. Encoding tespiti, BOM temizliÄŸi ve satÄ±r normalizasyonu gibi alt adÄ±mlar bu sÃ¼recin ayrÄ±lmaz parÃ§alarÄ±dÄ±r. Pythonâ€™un Unicode temelli mimarisi bu adÄ±mÄ± zorunlu kÄ±lar.
>âœ… **Girinti:** Pythonâ€™da sÃ¼slÃ¼ parantezler yerine, kod bloklarÄ±nÄ± tanÄ±mlamak iÃ§in kullanÄ±lan boÅŸluk veya tab karakterlerinden oluÅŸan yapÄ±sal bir iÅŸarettir
```python
# Girinti;
def greek():
    pass # burda 4 boÅŸluk var
```

---
>âœ… **Blok SÄ±nÄ±rÄ±:** Girinti seviyesinin deÄŸiÅŸmesiyle belirlenen yapÄ±sal geÃ§iÅŸ noktasÄ±dÄ±r
```python
# Blok SÄ±nÄ±rlarÄ±
for _ in range(10): # iki nokta for bloÄŸunun baÅŸladÄ±ÄŸÄ±nÄ± belirtir
    pass
```
---
>âœ… **Ä°fade Sonu:** Her Python ifadesi satÄ±r sonuyla (\n) veya noktalÄ± virgÃ¼lle (;) sonlanÄ±r
```python
# Ä°fade SonlarÄ±
name = "demir" # ifade sonu
age = 20 # ifade sonu
id = 12 ; city = istanbul # birden fazla ifadeyi : ile ayÄ±rdÄ±k
```
---
>ğŸ’¡ Python'da sÃ¼slÃ¼ parantezler yerine girintiler blok yapÄ±sÄ±nÄ± tanÄ±mlar; bu yÃ¼zden **lexer**, satÄ±r sonlarÄ±nÄ± ve boÅŸluk karakterlerini yapÄ±sal ayrÄ±m iÃ§in semantik olarak iÅŸler. Kaynak edinimi sÄ±rasÄ±nda satÄ±r sonlarÄ±nÄ±n normalize edilmemesi, **lexerâ€™Ä±n** girinti seviyelerini yanlÄ±ÅŸ yorumlamasÄ±na ve `IndentationError` gibi yapÄ±sal hatalara yol aÃ§abilir.


### âš™ï¸ CPython DÃ¼zeyinde SatÄ±r SonlarÄ±nÄ±n Normalize Edilmesi

SatÄ±r sonlarÄ±nÄ±n normalize edilmesi iÅŸlemi, Python yorumlayÄ±cÄ±sÄ±nÄ±n (CPython) **tokenizer** modÃ¼lÃ¼nde, yani `Python/tokenizer.c` dosyasÄ±nda gerÃ§ekleÅŸir.  
Bu iÅŸlem **lexer baÅŸlamadan hemen Ã¶nce**, kaynak metin â€œokuma akÄ±ÅŸÄ±â€ (readline stream) haline getirilirken yapÄ±lÄ±r.


