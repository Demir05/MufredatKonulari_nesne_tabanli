# ============================================================
# ğŸ” LEXER
# ============================================================
# 1) Genel TanÄ±m:
#    - Lexer (Lexical Analyzer), kaynak kodu en kÃ¼Ã§Ã¼k anlamlÄ± birimlere (tokenâ€™lara) ayÄ±ran araÃ§tÄ±r.
#    - AmacÄ±, parserâ€™Ä±n iÅŸini kolaylaÅŸtÄ±rmak iÃ§in ham karakterleri sÄ±nÄ±flandÄ±rmaktÄ±r.
#
# 2) Ä°leri TanÄ±mlar:
#    - Lexer, genellikle "finite state machine (FSM)" mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸÄ±r.
#      Karakterleri okur, durumlar arasÄ±nda geÃ§iÅŸ yapar, token sÄ±nÄ±rlarÄ±nÄ± belirler.
#    - Tokenizer ile farkÄ±: Tokenizer yalnÄ±zca bÃ¶ler, Lexer hem bÃ¶ler hem de sÄ±nÄ±flandÄ±rÄ±r.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Her derleyici ve yorumlayÄ±cÄ±da ilk aÅŸama.
#    - Kaynak kod analizi, statik kod kontrolleri, gÃ¼venlik taramalarÄ±.
#    - Ã–zel diller / DSL (Domain Specific Language) geliÅŸtirirken.
#
# 4) Ekstra Detaylar / Dikkat Edilecekler:
#    - Yorum satÄ±rlarÄ±, boÅŸluklar ve tab karakterleri genelde atÄ±lÄ±r.
#    - Python gibi dillerde girinti (INDENT / DEDENT) bile tokenâ€™a dÃ¶nÃ¼ÅŸebilir.
#    - String literal'ler, yorumlar ve Ã§ok karakterli operatÃ¶rler Ã¶zel iÅŸlenmelidir.
#
# 5) Ã–rnek:
#    Kod:   x = 5 + 3
#    Lexer Ã‡Ä±ktÄ±sÄ±: [NAME("x"), OP("="), NUMBER("5"), OP("+"), NUMBER("3")]
# ============================================================


# ============================================================
# ğŸ” TOKEN
# ============================================================
# 1) Genel TanÄ±m:
#    - Token, kaynak kodun en kÃ¼Ã§Ã¼k anlamlÄ± birimidir.
#    - TÃ¼r (type) + deÄŸer (value) + konum bilgisi (line, column) iÃ§erir.
#
# 2) Ä°leri TanÄ±mlar:
#    - Token tÃ¼rleri genellikle: KEYWORD, IDENTIFIER, NUMBER, STRING, OPERATOR, DELIMITER, NEWLINE.
#    - Python tokenize modÃ¼lÃ¼ her token iÃ§in "TokenInfo" nesnesi dÃ¶ndÃ¼rÃ¼r.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Parserâ€™Ä±n Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli girdiyi saÄŸlar.
#    - Kod renklendirme (syntax highlighting).
#    - Statik analiz araÃ§larÄ± (Ã¶r. pylint) token dÃ¼zeyinde tarama yapabilir.
#
# 4) Ekstra Detaylar:
#    - AynÄ± sembol farklÄ± baÄŸlamlarda farklÄ± token olabilir (Ã¶rn. "=" atama iken "==" karÅŸÄ±laÅŸtÄ±rma).
#    - String literal iÃ§inde geÃ§en semboller tokenâ€™a ayrÄ±lmaz.
#
# 5) Ã–rnek:
#    Kod: print("x=5")
#    Tokenler: [NAME("print"), OP("("), STRING("x=5"), OP(")")]
# ============================================================


# ============================================================
# ğŸ” TOKENIZER
# ============================================================
# 1) Genel TanÄ±m:
#    - Tokenizer, ham karakter akÄ±ÅŸÄ±nÄ± dilin tanÄ±mlÄ± kurallarÄ±na gÃ¶re parÃ§alara bÃ¶ler.
#
# 2) Ä°leri TanÄ±mlar:
#    - Tokenizer genelde lexerâ€™in bir alt parÃ§asÄ± olarak Ã§alÄ±ÅŸÄ±r.
#    - AyrÄ±ÅŸtÄ±rma yapar ama sÄ±nÄ±flandÄ±rmayÄ± lexer tamamlar.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Dil iÅŸleme (compilers, interpreters).
#    - Veri iÅŸleme (Ã¶r. CSV, JSON parser'larÄ± bile basit tokenizasyon yapar).
#
# 4) Ekstra Detaylar:
#    - Tokenizer yalnÄ±zca sÄ±nÄ±rlarÄ± bulur, semantic anlam vermez.
#    - Ã–rn: "123" sadece "sayÄ± dizisi" olarak ayrÄ±lÄ±r, bunun "NUMBER token" olduÄŸunu lexer karar verir.
#
# 5) Ã–rnek:
#    Kod: abc123
#    Tokenizer Ã‡Ä±ktÄ±sÄ±: ["abc123"]
#    Lexer Ã‡Ä±ktÄ±sÄ±: [IDENTIFIER("abc123")]
# ============================================================


# ============================================================
# ğŸ” REGEX Ä°LE Ä°LÄ°ÅKÄ°
# ============================================================
# 1) Genel TanÄ±m:
#    - Lexer, karakter Ã¶rÃ¼ntÃ¼lerini tanÄ±mlamak iÃ§in regex benzeri kurallardan faydalanÄ±r.
#
# 2) Ä°leri TanÄ±mlar:
#    - Her token tÃ¼rÃ¼ bir regex ile tarif edilebilir:
#        IDENTIFIER â†’ [a-zA-Z_][a-zA-Z0-9_]*
#        NUMBER     â†’ [0-9]+
#        STRING     â†’ ".*?"
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Lexer yazarken regex en hÄ±zlÄ± Ã§Ã¶zÃ¼m.
#    - Token sÄ±nÄ±rlarÄ±nÄ± net ÅŸekilde tanÄ±mlamak iÃ§in.
#
# 4) Ekstra Detaylar:
#    - Regex sadece deseni tanÄ±mlar, "bu desen IDENTIFIERâ€™dÄ±r" yorumunu lexer yapar.
#    - BÃ¼yÃ¼k dillerde lexer genelde regex yerine optimize edilmiÅŸ state machine kullanÄ±r.
#
# 5) Ã–rnek:
#    Kod: var1 = 99
#    Regex ile eÅŸleÅŸmeler: [IDENTIFIER("var1"), OP("="), NUMBER("99")]
# ============================================================


# ============================================================
# ğŸ” EBNF (Extended Backusâ€“Naur Form) Ä°LE Ä°LÄ°ÅKÄ°
# ============================================================
# 1) Genel TanÄ±m:
#    - EBNF, bir dilin gramerini tanÄ±mlamak iÃ§in kullanÄ±lan formal bir notasyondur.
#
# 2) Ä°leri TanÄ±mlar:
#    - Lexer seviyesinde: hangi karakter Ã¶rÃ¼ntÃ¼lerinin token oluÅŸturacaÄŸÄ±nÄ± tarif eder.
#    - Parser seviyesinde: tokenâ€™larÄ±n nasÄ±l birleÅŸip dil yapÄ±sÄ±nÄ± oluÅŸturacaÄŸÄ±nÄ± tanÄ±mlar.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Programlama dillerinin resmi spesifikasyonlarÄ± (Ã¶rn. Python grammar).
#    - Parser jeneratÃ¶rleri (ANTLR, yacc, bison).
#
# 4) Ekstra Detaylar:
#    - EBNF ile regex arasÄ±ndaki fark: Regex â†’ dÃ¼ÅŸÃ¼k seviye desenler, EBNF â†’ dilin kurallarÄ±.
#    - EBNF, lexer ve parser arasÄ±ndaki kÃ¶prÃ¼ gibidir.
#
# 5) Ã–rnek:
#    EBNF kuralÄ±:
#        assignment = identifier "=" expression ;
#        expression = number | expression "+" expression ;
#
#    Kod: x = 5 + 3
#    Tokenler: [NAME("x"), OP("="), NUMBER("5"), OP("+"), NUMBER("3")]
#    Parser â†’ AST: Assign(Name("x"), BinOp(5, Add, 3))
# ============================================================
