# ============================================================
# ğŸ” REGEX TABANLI LEXER
# ============================================================
# 1) Genel TanÄ±m:
#    - Token kurallarÄ± regex ifadeleriyle tanÄ±mlanÄ±r.
#    - Input string Ã¼zerinde bu regexâ€™ler uygulanÄ±r, eÅŸleÅŸmelere gÃ¶re token Ã¼retilir.
#
# 2) Ä°leri TanÄ±mlar:
#    - Lexer her token tipi iÃ§in bir regex tanÄ±mlar:
#         NUMBER â†’ "\d+"
#         IDENT  â†’ "[a-zA-Z_][a-zA-Z0-9_]*"
#    - Kaynak kodda sÄ±rayla bu patternâ€™ler eÅŸleÅŸir, token listesi oluÅŸturulur.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - KÃ¼Ã§Ã¼k diller veya DSL (domain specific language).
#    - EÄŸitim amaÃ§lÄ± derleyici tasarÄ±mÄ±.
#    - Prototipleme (hÄ±zlÄ±ca lexer yazmak iÃ§in).
#
# 4) Ekstra Detaylar:
#    - KolaydÄ±r ama performansÄ± zayÄ±ftÄ±r (regex motoru ileri/geri bakÄ±ÅŸ yapabilir).
#    - BÃ¼yÃ¼k dillerde regex tabanlÄ± lexer yetersiz kalÄ±r â†’ yerine DFA/NFA tabanlÄ± lexer kullanÄ±lÄ±r.
#
# 5) Ã–rnek:
#    >>> TOKENS = [("NUMBER", r"\d+"), ("PLUS", r"\+"), ("IDENT", r"[a-zA-Z_]\w*")]
#    >>> "10 + x" â†’ [('NUMBER', '10'), ('PLUS', '+'), ('IDENT', 'x')]
# ============================================================


# ============================================================
# ğŸ” INDENT, DEDENT, NEWLINE (Girinti tabanlÄ± dillerde Ã¶zel tokenler)
# ============================================================
# 1) Genel TanÄ±m:
#    - Ã‡oÄŸu dilde blok yapÄ±sÄ± {} ile belirlenir, Pythonâ€™da girinti ile.
#    - Bu yÃ¼zden lexer, girinti deÄŸiÅŸikliklerini Ã¶zel tokenâ€™lara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:
#         INDENT  â†’ girinti baÅŸladÄ±
#         DEDENT  â†’ girinti bitti
#         NEWLINE â†’ satÄ±r sonu
#
# 2) Ä°leri TanÄ±mlar:
#    - Tokenizer girinti seviyesini bir stack Ã¼zerinde takip eder.
#    - Yeni bir satÄ±rda girinti artarsa INDENT token, azalÄ±rsa DEDENT token Ã¼retir.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Sadece Python gibi indentation-based dillerde.
#    - Parserâ€™Ä±n blok yapÄ±sÄ±nÄ± anlamasÄ± iÃ§in zorunludur.
#
# 4) Ekstra Detaylar:
#    - Bu tokenâ€™lar olmadan parser, hangi ifadelerin aynÄ± blokta olduÄŸunu bilemez.
#    - DiÄŸer dillerde bu iÅŸ sÃ¼slÃ¼ parantez "{" "}" veya "begin-end" ile yapÄ±lÄ±r.
#
# 5) Ã–rnek:
#    Kod:
#        if True:
#            x = 5
#    Tokenler:
#        NAME("if"), NAME("True"), OP(":"), NEWLINE,
#        INDENT, NAME("x"), OP("="), NUMBER("5"), NEWLINE, DEDENT
# ============================================================


# ============================================================
# ğŸ” LEXER OPTÄ°MÄ°ZASYONLARI (Regex vs DFA/NFA)
# ============================================================
# 1) Genel TanÄ±m:
#    - Lexer implementasyonunda iki yaklaÅŸÄ±m vardÄ±r:
#        a) Regex tabanlÄ± lexer
#        b) State machine tabanlÄ± lexer (DFA/NFA)
#
# 2) Ä°leri TanÄ±mlar:
#    - Regex tabanlÄ± lexer:
#        â€¢ Token kurallarÄ± regex ile yazÄ±lÄ±r.
#        â€¢ Motor ileri/geri bakÄ±ÅŸ yapabilir, maliyetlidir.
#    - DFA/NFA tabanlÄ± lexer:
#        â€¢ Deterministic Finite Automaton (DFA) mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸÄ±r.
#        â€¢ Karakterleri tek geÃ§iÅŸte iÅŸler, daha hÄ±zlÄ±dÄ±r.
#
# 3) KullanÄ±m AlanlarÄ±:
#    - Regex tabanlÄ± lexer â†’ kÃ¼Ã§Ã¼k diller, eÄŸitim, prototip.
#    - DFA/NFA lexer â†’ bÃ¼yÃ¼k diller (C, Python, Java).
#
# 4) Ekstra Detaylar:
#    - CPythonâ€™un tokenizer.c dosyasÄ± DFA tabanlÄ±dÄ±r.
#    - DFA: Tek geÃ§iÅŸte karar verir, bellek dostudur.
#    - Regex motoru: Ã‡ok geÃ§iÅŸli olabilir, bellek kullanÄ±mÄ± daha fazladÄ±r.
#
# 5) Ã–rnek:
#    Regex tabanlÄ± lexer:
#        "\d+" â†’ "123" (NUMBER)
#    DFA tabanlÄ± lexer:
#        Durumlar: START â†’ DIGIT â†’ NUMBER_ACCEPT
#        "123" â†’ NUMBER token (tek geÃ§iÅŸte)
# ============================================================